{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo and cd into repo directory\n",
    "!git clone https://github.com/naafidasana/NativLang.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "%cd NativLang/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive, files\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gpt2_model import GPTModel, GPTConfig\n",
    "from utils.general_utils import Visualizer, MetricAccumulator, Timer, get_gpus, download_and_extract\n",
    "from utils.dataloaders import get_gpt_batch, read_data_for_gpt\n",
    "from utils.tokenizer import BPETokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (or fetched cached dataset) and obtain path to dataset\n",
    "DATA_DIR = \"./data/dag-sents-train\"\n",
    "text = read_data_for_gpt(DATA_DIR)\n",
    "\n",
    "# Get pretreained tokenizer\n",
    "tokenizer = BPETokenizer.from_pretrained(\"configs/dagpt-base-uncased-tokenizer.json\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Encode text\n",
    "enc_text = np.array(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data torch data loaders and vocabulary\n",
    "context_length = 128\n",
    "batch_size = 52\n",
    "\n",
    "# Create model config and model\n",
    "config = GPTConfig(vocab_size+1, context_length)\n",
    "\n",
    "# Get device\n",
    "devices, num_devices = get_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_model(config, checkpoint_path=None):\n",
    "  if checkpoint_path is not None:\n",
    "    # Initialize from checkpoint\n",
    "    print(f\"Loading from {checkpoint_path.split('/')[-1]} ...\")\n",
    "    model = GPTModel.from_pretrained(checkpoint_path, config)\n",
    "  else:\n",
    "    model = GPTModel(config)\n",
    "  return model.to(devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(model, theta):\n",
    "    params = [p for p in model.parameters() if p.requires_grad()]\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2) for p in params)))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "\n",
    "def get_gpt_batch_loss(model, input_sequences, targets):\n",
    "    # Loss is also calculated in the forward pass of the model\n",
    "    logits, loss = model(input_sequences, targets)\n",
    "    return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt(config, model, train_data, learning_rate, num_steps):\n",
    "\n",
    "    # Add code to enable checkpointing\n",
    "    checkpoint_path = \"./checkpoints\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.mkdir(checkpoint_path)\n",
    "\n",
    "    # Get and move model to device.\n",
    "    if num_devices >= 2:\n",
    "        # Use parallel processing on multiple GPUs.\n",
    "        model = nn.DataParallel(model, device_ids=devices).to(devices[0])\n",
    "    else:\n",
    "        # Still explicitly move model to device incase there is only one GPU.\n",
    "        model.to(devices[0])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    step, timer = 0, Timer()\n",
    "    visualizer = Visualizer(xlabel=\"step\", ylabel=\"loss\",\n",
    "                            xlim=[1, num_steps])\n",
    "    metrics = MetricAccumulator(3)\n",
    "\n",
    "    print(\"Training...\")\n",
    "    while step < num_steps:\n",
    "        # Get batch inputs and targets\n",
    "        batch_xs, batch_ys = get_gpt_batch(train_data, config.context_length, batch_size)\n",
    "        batch_xs, batch_ys = batch_xs.to(devices[0], non_blocking=True), batch_ys.to(devices[0], non_blocking=True)\n",
    "\n",
    "        # Reset any previously computed gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Do forward pass and fetch loss\n",
    "        timer.start()\n",
    "        loss = get_gpt_batch_loss(model, batch_xs, batch_ys)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        #grad_clipping(model, 1)\n",
    "        optimizer.step()\n",
    "        metrics.add(loss,\n",
    "                    batch_xs.shape[0], 1)\n",
    "        timer.stop()\n",
    "        visualizer.add(step + 1, metrics[0]/metrics[2])\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # Save model after every 300 steps\n",
    "        # We process `batch_size` number of examples in each step\n",
    "        if (step % 1000 == 0):\n",
    "            checkpoint_name = os.path.join(checkpoint_path, f\"dagpt-v3-{step:03}.pth\")\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), checkpoint_name)\n",
    "\n",
    "            # Print generated sequence\n",
    "            #seq = \"ŋɔ wuhirila niriba\"\n",
    "            #try_generate(model, seq, max_tokens=10)\n",
    "\n",
    "\n",
    "            print(f\"Loss: {metrics[0]/metrics[2]:.4f}\")\n",
    "            print(f\"{metrics[1]/timer.sum():.1f} tokens/sec on {str(devices)}\")\n",
    "\n",
    "            # Copy generated checkpoints to google drive folder\n",
    "            destination_folder = \"/content/drive/My Drive/NativLang/DaGPT\"\n",
    "            source_file = f\"checkpoints/dagpt-v3-{step}.pth\"\n",
    "\n",
    "            shutil.copy(source_file, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_generate(model, seq, max_tokens=10):\n",
    "    # Encode sequence\n",
    "    encoded_seq = tokenizer.encode(seq)\n",
    "    encoded_seq = torch.tensor(encoded_seq).to(devices[0]).unsqueeze(0)\n",
    "    encoded_seq = encoded_seq.to(devices[0])\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            gen_seq = model.module.generate(encoded_seq, max_tokens)\n",
    "        except:\n",
    "            gen_seq = model.generate(encoded_seq, max_tokens)\n",
    "        gen_seq = gen_seq.cpu().numpy().tolist()[0]\n",
    "        # Decode generated sequence\n",
    "        print(tokenizer.decode(gen_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "checkpoint_path = \"/content/drive/My Drive/NativLang/DaGPT/dagpt-v3-5000\"\n",
    "#checkpoint_path = \"/content/drive/Shared with me/NativLang/DaGPT/dagpt-v2-5000\"\n",
    "model = fetch_model(checkpoint_path=checkpoint_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of Parameters: {sum([p.numel() for p in model.parameters()])/1e6} M\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify learning rate and num_epochs, and train model\n",
    "lr, num_steps = 3e-5, 5000\n",
    "train_gpt(config, model, train_data=enc_text, learning_rate=lr, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example generations.\n",
    "seqs = [\"di nyɛla shikuru\", \"Laɣingu maa yɛltɔɣa kpani daa nyɛla\"]\n",
    "for seq in seqs:\n",
    "  gen_seq = try_generate(model, seq, max_tokens=40)\n",
    "  print(\"=\"*150)\n",
    "  print(gen_seq)\n",
    "  print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
